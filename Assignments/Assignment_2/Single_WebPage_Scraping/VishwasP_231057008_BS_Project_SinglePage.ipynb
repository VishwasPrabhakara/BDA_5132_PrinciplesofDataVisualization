{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \n",
    "1. question,\n",
    "2. question link to access that question, \n",
    "3. view count,\n",
    "4. votes count, \n",
    "5. number of answers,\n",
    "6. the username of the person who posted the question on the StackOverflow Website for a single page\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Stack Overflow Website using Requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = UserAgent()\n",
    "main_url = \"https://stackoverflow.com/questions/tagged/python\" #main url or the base url\n",
    "\n",
    "page = requests.get(main_url,user_agent.chrome)\n",
    "soup = BeautifulSoup(page.text,'lxml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data using tags and storing it in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = [] #list to store the scraped data\n",
    "\n",
    "all_divs = soup.find_all('div',class_='s-post-summary') \n",
    "for div in all_divs:\n",
    "    question_link = f\"https://stackoverflow.com/\"+ div.find('a').get('href')\n",
    "    question = div.find('h3',{'class':'s-post-summary--content-title'}).text.strip('\\n')\n",
    "    votes_count = div.find('div',{'class':'s-post-summary--stats-item s-post-summary--stats-item__emphasized'}).text.split('\\n')[1] + \" votes\"\n",
    "    answers_count = div.find_all('span',{'class':'s-post-summary--stats-item-number'})[1].text + \" answers\"\n",
    "    views_count = div.find_all('span',{'class':'s-post-summary--stats-item-number'})[2].text + \" views\"\n",
    "    user_name = div.find('div',{'class':'s-user-card--link'}).contents[1].text\n",
    "    \n",
    "    data = [question_link,question,votes_count,answers_count,views_count,user_name]\n",
    "    scraped_data.append(data)   #appending the scraped data to the list \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Storing the scraped data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_df = pd.DataFrame(scraped_data,columns=['Question Link','Question','Votes Count','Answers Count','Views Count','Username']) \n",
    "#creating a pandas dataframe as pandas has builtin function to convert the data to csv file\n",
    "scraped_data_df.index+=1\n",
    "\n",
    "scraped_data_df.to_csv('Stack_Questions.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaretenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
